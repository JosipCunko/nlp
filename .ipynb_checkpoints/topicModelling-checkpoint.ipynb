{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb6b39f-0d3c-4299-8e08-102252a66eeb",
   "metadata": {},
   "source": [
    "# Latent dirichlet allocation\n",
    "- focus on a single topic\n",
    "- iterative\n",
    "- proportion of words in the current doc\n",
    "- number of times the word is assigned to a topic in other docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ffcb1c-bc56-4b3c-a0d4-3242d74476c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as wordTokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "import gensim.corpora as corpora # text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8616bc-f505-4e96-b2ba-812ab82bfc63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25626</td>\n",
       "      <td>One Weight-Loss Approach Fits All? No, Not Eve...</td>\n",
       "      <td>Dr. Frank Sacks, a professor of nutrition at H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19551</td>\n",
       "      <td>South Carolina Stuns Baylor to Reach the Round...</td>\n",
       "      <td>South Carolina’s win over   Duke was not only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25221</td>\n",
       "      <td>U.S. Presidential Race, Apple, Gene Wilder: Yo...</td>\n",
       "      <td>(Want to get this briefing by email? Here’s th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18026</td>\n",
       "      <td>His Predecessor Gone, Gambia’s New President F...</td>\n",
       "      <td>BANJUL, Gambia  —   A week after he was inaugu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21063</td>\n",
       "      <td>‘Harry Potter and the Cursed Child’ Goes From ...</td>\n",
       "      <td>The biggest book of the summer isn’t a blockbu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  25626  One Weight-Loss Approach Fits All? No, Not Eve...   \n",
       "1  19551  South Carolina Stuns Baylor to Reach the Round...   \n",
       "2  25221  U.S. Presidential Race, Apple, Gene Wilder: Yo...   \n",
       "3  18026  His Predecessor Gone, Gambia’s New President F...   \n",
       "4  21063  ‘Harry Potter and the Cursed Child’ Goes From ...   \n",
       "\n",
       "                                             content  \n",
       "0  Dr. Frank Sacks, a professor of nutrition at H...  \n",
       "1  South Carolina’s win over   Duke was not only ...  \n",
       "2  (Want to get this briefing by email? Here’s th...  \n",
       "3  BANJUL, Gambia  —   A week after he was inaugu...  \n",
       "4  The biggest book of the summer isn’t a blockbu...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare text\n",
    "data = pd.read_csv(\"news_articles.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432b2213-59b1-42c2-9bc4-0b0339aa9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       100 non-null    int64 \n",
      " 1   title    100 non-null    object\n",
      " 2   content  100 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc7c287-9d10-418c-adda-1bbb092c9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955342d9-b92c-41a1-881e-d29df54b8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.str.lower().apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7bce4f8-2d96-48d1-a6f8-e7a7f8261d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enStopwords = stopwords.words(\"english\")\n",
    "articles = articles.apply(lambda content: \" \".join([word for word in content.split(\" \") if word not in enStopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755e5169-222b-47dc-b22f-acc975bbde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.apply(lambda content: wordTokenize(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f787653d-a4c3-43ad-a429-eb52ebd8b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# done for speed over lemmatizing\n",
    "ps = PorterStemmer()\n",
    "articles = articles.apply(lambda tokens: [ps.stem(token) for token in tokens]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc52ccce-0a77-41e4-8dc7-32827d680cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [dr, frank, sack, professor, nutrit, harvard, ...\n",
       "1     [south, carolina, win, duke, surpris, fan, pos...\n",
       "2     [want, get, brief, email, here, good, even, he...\n",
       "3     [banjul, gambia, week, inaugur, anoth, countri...\n",
       "4     [biggest, book, summer, isnt, blockbust, thril...\n",
       "                            ...                        \n",
       "95    [want, get, brief, email, here, good, even, he...\n",
       "96    [tallinn, estonia, guard, brought, ahm, abdul,...\n",
       "97    [gov, scott, walker, wisconsin, activ, wiscons...\n",
       "98    [social, media, shook, emot, headlin, shout, n...\n",
       "99    [moment, joanna, acevedo, first, set, foot, bo...\n",
       "Name: content, Length: 100, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de99ad68-897f-4e6a-921c-356c8bb1a40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1c7681940d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the structure that LDA expects\n",
    "dictionary = corpora.Dictionary(articles)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0a0bb-0522-42b1-8f9b-953398570225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to vectorizing\n",
    "docTerm = [dictionary.doc2bow(text) for text in articles]\n",
    "#print(docTerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674c0352-65b2-4714-917e-47dd295be067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "numTopics = 2\n",
    "ldaModel = gensim.models.LdaModel(\n",
    "    corpus = docTerm, # matrix\n",
    "    id2word = dictionary, # maps word ids back to actual words\n",
    "    num_topics = numTopics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580f4087-b087-42ed-af5b-90514ddf6c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"mr\" + 0.012*\"said\" + 0.005*\"trump\" + 0.005*\"would\" + 0.004*\"year\"'),\n",
       " (1,\n",
       "  '0.020*\"mr\" + 0.017*\"said\" + 0.006*\"trump\" + 0.005*\"state\" + 0.004*\"one\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaModel.print_topics(num_topics = numTopics, num_words = 5)\n",
    "# most important words for every topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b394fc-1b0d-4a8b-8556-ff3e05cf26b4",
   "metadata": {},
   "source": [
    "# Latent semantic analysis\n",
    "- other method for topic modelling\n",
    "- words with similar meaning appear frequently together\n",
    "- singular value decomposition - recreates text docs into diff vectors\n",
    "- method of dimensionality reduction : similarity because of clustering and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "702a830f-4c9b-4b13-9c45-f46a04f026ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78b53614-d109-4ee4-9d15-82adb6613289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.615*\"mr\" + 0.429*\"said\" + 0.187*\"trump\" + 0.130*\"state\" + 0.119*\"would\"'),\n",
       " (1,\n",
       "  '-0.537*\"mr\" + -0.319*\"trump\" + 0.286*\"said\" + 0.242*\"saudi\" + 0.142*\"weight\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsi model = lsa model\n",
    "lsaModel = LsiModel(docTerm, num_topics=numTopics, id2word = dictionary)\n",
    "lsaModel.print_topics(numTopics, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67440788-a54d-4b5f-a96e-454d755b9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize the number of topics\n",
    "# coherent score => how meaningful the top words in a topic are when grouped together\n",
    "# higher score => more sense to humans\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ff844-0a68-4ff9-8c7e-07890e543ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherenceValues = []\n",
    "modelList = []\n",
    "minTopics =2\n",
    "maxTopics = 11 # choose\n",
    "\n",
    "#random seed => same result every time code runs\n",
    "for numTopicsIndex in range(minTopics, maxTopics + 1):\n",
    "    model = LsiModel(docTerm, num_topics=numTopicsIndex, id2word=dictionary, random_seed = 0)\n",
    "    modelList.append(model)\n",
    "    # coherence => how often the top words actually appear together in the docs\n",
    "    coherenceModel = CoherenceModel(model=model, texts=articles, dictionary=dictionary, coherence=\"c_v\")\n",
    "    coherenceValues.append(coherenceModel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a80001-0614-4450-9815-19a371867a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(minTopics, maxTopics+1), coherenceValues)\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherenceValues\"), loc=\"best\")\n",
    "plt.show()\n",
    "# model with 3 topics gives the most meaningful grouping of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147c0cd-31d5-4b0a-a4c7-87b8315216da",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalNumTopic = 3\n",
    "lsaModelFinal = LsiModel(docTerm, num_topics=finalNumTopic, id2word = dictionary)\n",
    "lsaModelFinal.print_topics(finalNumTopic, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1711da-8e74-41f6-89ca-de02b52cb2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy-nlp",
   "language": "python",
   "name": "udemy-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
